# -*- coding: utf-8 -*-
"""pou-assigmnets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMVS_GpeRQoLheuPvI8zjxtdITWqdl_T
"""

# 1. Load and Inspect the Dataset
import pandas as pd

# Load My dataset
df = pd.read_csv("creditcard.csv")
print("Shape of dataset:", df.shape)
df.head()

# 2. Balancing the Dataset
import seaborn as sns
import matplotlib.pyplot as plt

# Check class balance
sns.countplot(x='Class', data=df)
plt.title("Class Distribution")
plt.xlabel("0: Not Fraud | 1: Fraud")
plt.show()

# 3. Preprocess the Data
from sklearn.preprocessing import StandardScaler

# Inspect the columns
print(df.columns)

# Drop 'Time' column and normalize 'Amount' column
df.drop("Time", axis=1, inplace=True)
df["Amount"] = StandardScaler().fit_transform(df[["Amount"]])

# 4. Create a Balanced Dataset
fraud = df[df['Class'] == 1]
non_fraud = df[df['Class'] == 0].sample(n=len(fraud)*5, random_state=42)
balanced_df = pd.concat([fraud, non_fraud])
X = balanced_df.drop("Class", axis=1)
y = balanced_df["Class"]

# 5. Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. Test Multiple Models to Select the Best One

# Logistic Regression
from sklearn.linear_model import LogisticRegression # Import Logistic Regression
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# Define parameter grid for XGBoost
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [4, 6],
    'learning_rate': [0.01, 0.1]
}

# Initialize and perform Grid Search for XGBoost
xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
                   param_grid=xgb_params, scoring='f1', cv=3)
xgb.fit(X_train, y_train)

# Initialize and train Logistic Regression model
logreg = LogisticRegression(random_state=42)
logreg.fit(X_train, y_train)

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# xgboost
from xgboost import XGBClassifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# 7. Compare Initial Model Performance
from sklearn.metrics import classification_report

print("Logistic Regression")
print(classification_report(y_test, logreg.predict(X_test)))

print("Random Forest")
print(classification_report(y_test, rf.predict(X_test)))

print("XGBoost")
print(classification_report(y_test, xgb_model.predict(X_test)))

# 8. Fine-Tune the Best Model (XGBoost)
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

y_pred = xgb.best_estimator_.predict(X_test)
y_prob = xgb.best_estimator_.predict_proba(X_test)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_prob))

# 9. Predict on a Random Transaction
sample = X_test.sample(1, random_state=7)
pred = xgb.best_estimator_.predict(sample)[0]
prob = xgb.best_estimator_.predict_proba(sample)[0][1]

print(f"Prediction: {'Fraud' if pred == 1 else 'Not Fraud'}")
print(f"Probability of Fraud: {prob:.2f}")