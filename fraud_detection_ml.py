# -*- coding: utf-8 -*-
"""fraud-detection-ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMVS_GpeRQoLheuPvI8zjxtdITWqdl_T
"""

# 1. Load and Inspect the Dataset
import pandas as pd

# Load My dataset
df = pd.read_csv("creditcard.csv")
print("Shape of dataset:", df.shape)
df.head()

# 2. Balancing the Dataset
import seaborn as sns
import matplotlib.pyplot as plt

# Check class balance
sns.countplot(x='Class', data=df)
plt.title("Class Distribution")
plt.xlabel("0: Not Fraud | 1: Fraud")
plt.show()

# 3. Preprocess the Data
from sklearn.preprocessing import StandardScaler

# Inspect the columns
print(df.columns)

# Drop 'Time' column and normalize 'Amount' column
df.drop("Time", axis=1, inplace=True)
df["Amount"] = StandardScaler().fit_transform(df[["Amount"]])

# 4. Create a Balanced Dataset
fraud = df[df['Class'] == 1]
non_fraud = df[df['Class'] == 0].sample(n=len(fraud)*5, random_state=42)
balanced_df = pd.concat([fraud, non_fraud])
X = balanced_df.drop("Class", axis=1)
y = balanced_df["Class"]

# 5. Split Data into Training and Testing Sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. Test Multiple Models to Select the Best One

# Logistic Regression
from sklearn.linear_model import LogisticRegression # Import Logistic Regression
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# Define parameter grid for XGBoost
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [4, 6],
    'learning_rate': [0.01, 0.1]
}

# Initialize and perform Grid Search for XGBoost
xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
                   param_grid=xgb_params, scoring='f1', cv=3)
xgb.fit(X_train, y_train)

# Initialize and train Logistic Regression model
logreg = LogisticRegression(random_state=42)
logreg.fit(X_train, y_train)

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# xgboost
from xgboost import XGBClassifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# 7. Compare Initial Model Performance

from sklearn.metrics import classification_report, roc_auc_score

# Logistic Regression
print("Logistic Regression")
print(classification_report(y_test, logreg.predict(X_test)))
logreg_probs = logreg.predict_proba(X_test)[:, 1]
logreg_roc_auc = roc_auc_score(y_test, logreg_probs)
print(f"ROC AUC: {logreg_roc_auc:.2f}\n")

# Random Forest
print("Random Forest")
print(classification_report(y_test, rf.predict(X_test)))
rf_probs = rf.predict_proba(X_test)[:, 1]
rf_roc_auc = roc_auc_score(y_test, rf_probs)
print(f"ROC AUC: {rf_roc_auc:.2f}\n")

# XGBoost
print("XGBoost")
print(classification_report(y_test, xgb_model.predict(X_test)))
xgb_probs = xgb_model.predict_proba(X_test)[:, 1]
xgb_roc_auc = roc_auc_score(y_test, xgb_probs)
print(f"ROC AUC: {xgb_roc_auc:.2f}\n")

# 8. Predict on a Random Transaction
import random
sample = X_test.sample(1, random_state=7)

# Logistic Regression
logreg_pred = logreg.predict(sample)[0]
logreg_prob = logreg.predict_proba(sample)[0][1]
print(f"Logistic Regression Prediction: {'Fraud' if logreg_pred == 1 else 'Not Fraud'}")
print(f"Probability of Fraud (Logistic Regression): {logreg_prob:.2f}")

# Random Forest
rf_pred = rf.predict(sample)[0]
rf_prob = rf.predict_proba(sample)[0][1]
print(f"Random Forest Prediction: {'Fraud' if rf_pred == 1 else 'Not Fraud'}")
print(f"Probability of Fraud (Random Forest): {rf_prob:.2f}")

# XGBoost
xgb_pred = xgb_model.predict(sample)[0]
xgb_prob = xgb_model.predict_proba(sample)[0][1]
print(f"XGBoost Prediction: {'Fraud' if xgb_pred == 1 else 'Not Fraud'}")
print(f"Probability of Fraud (XGBoost): {xgb_prob:.2f}")